from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

model_id = "mistralai/Mistral-7B-Instruct-v0.1"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype="auto"
)

pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)

review = "ì´ ì‹ë‹¹ì€ ë¶„ìœ„ê¸°ê°€ ë„ˆë¬´ ì¢‹ê³ , ìŒì‹ì´ ì •ë§ ë§›ìˆì–´ìš”. ì§ì›ë“¤ë„ ì¹œì ˆí•´ì„œ ë‹¤ìŒì— ë˜ ì˜¤ê³  ì‹¶ë„¤ìš”."

prompt = f"""ì•„ë˜ ë¦¬ë·°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ëœ í•´ì‹œíƒœê·¸ë¥¼ 5ê°œ ì¶”ì²œí•´ì¤˜.
ë¦¬ë·°: "{review}"

í˜•ì‹: #í•´ì‹œíƒœê·¸1 #í•´ì‹œíƒœê·¸2 #í•´ì‹œíƒœê·¸3 ...
"""

output = pipe(prompt, max_new_tokens=50, do_sample=True, temperature=1.2)
print("ğŸ¯ ìƒì„±ëœ í•´ì‹œíƒœê·¸:", output[0]['generated_text'])
